# Lean4 Delethink æ•°æ®å‡†å¤‡å·¥å…·

å°† Lean4 è¯æ˜åˆ‡åˆ†æˆå—ï¼Œæ„é€ ç”¨äºè®­ç»ƒ"åˆ†å—æ¨ç†"æ¨¡å‹çš„æ•°æ®é›†ã€‚

## ğŸ“‹ åŠŸèƒ½

- âœ… ä¸‹è½½ MiniF2F-Lean4 æ•°æ®é›†
- âœ… ä¸‰ç§åˆ‡åˆ†ç­–ç•¥ï¼šæŒ‰è¡Œã€æŒ‰è¯­æ³•ã€æŒ‰token
- âœ… è‡ªåŠ¨æ„é€ è®­ç»ƒæ ·æœ¬ï¼ˆæ¨¡æ‹Ÿ Delethink æ¨ç†æµç¨‹ï¼‰
- âœ… æ•°æ®ç»Ÿè®¡å’Œå¯è§†åŒ–
- âœ… ä¸€é”®è¿è¡Œå®Œæ•´æµç¨‹

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
cd code
pip install -r requirements.txt
```

### 2. é…ç½®å‚æ•°

ç¼–è¾‘ `config.yaml` è°ƒæ•´é…ç½®ï¼š

```yaml
# åˆ‡åˆ†ç­–ç•¥
chunking:
  strategy: "line_based"  # å¯é€‰: line_based, syntax_based, token_based

  line_based:
    target_chunks: 3      # æ¯ä¸ªè¯æ˜åˆ‡æˆå‡ å—
    min_lines_per_chunk: 5
    max_lines_per_chunk: 50

# Delethink å‚æ•°
delethink:
  keep_head: 100          # ä¿ç•™å¤´éƒ¨è¡Œæ•°
  keep_tail: 20           # ä¿ç•™å°¾éƒ¨è¡Œæ•°
```

### 3. è¿è¡Œæµç¨‹

#### æ–¹å¼Aï¼šä¸€é”®è¿è¡Œï¼ˆæ¨èï¼‰

```bash
python run_pipeline.py --steps all
```

#### æ–¹å¼Bï¼šåˆ†æ­¥è¿è¡Œ

```bash
# æ­¥éª¤1: ä¸‹è½½æ•°æ®
python run_pipeline.py --steps download

# æ­¥éª¤2: æ„é€ è®­ç»ƒæ•°æ®
python run_pipeline.py --steps build

# æ­¥éª¤3: æ•°æ®åˆ†æ
python run_pipeline.py --steps analyze
```

#### æ–¹å¼Cï¼šæŒ‡å®šç­–ç•¥

```bash
# ä½¿ç”¨æŒ‰è¯­æ³•åˆ‡åˆ†
python run_pipeline.py --strategy syntax_based

# ä½¿ç”¨æŒ‰tokenåˆ‡åˆ†
python run_pipeline.py --strategy token_based
```

### 4. è¾“å‡ºæ–‡ä»¶

```
data/
â”œâ”€â”€ raw/                          # åŸå§‹ä¸‹è½½æ•°æ®
â”‚   â”œâ”€â”€ valid_raw.jsonl
â”‚   â”œâ”€â”€ valid_filtered.jsonl
â”‚   â””â”€â”€ test_...
â”‚
â””â”€â”€ processed/                    # è®­ç»ƒæ•°æ®
    â”œâ”€â”€ train.jsonl              # è®­ç»ƒé›† â­
    â”œâ”€â”€ val.jsonl                # éªŒè¯é›† â­
    â”œâ”€â”€ valid_training.jsonl
    â”œâ”€â”€ test_training.jsonl
    â””â”€â”€ plots/                   # ç»Ÿè®¡å›¾è¡¨
        â”œâ”€â”€ chunks_per_proof.png
        â”œâ”€â”€ chunk_lengths.png
        â””â”€â”€ length_distributions.png
```

## ğŸ“Š æ•°æ®æ ¼å¼

### è¾“å…¥æ ¼å¼ï¼ˆåŸå§‹æ•°æ®ï¼‰

```json
{
  "id": 0,
  "theorem": "theorem add_comm (a b : â„•) : a + b = b + a := by",
  "proof": "  intro a b\n  induction a with\n  | zero => simp\n  | succ n ih => ...",
  "informal_statement": "Prove that addition is commutative",
  "informal_proof": "By induction on a..."
}
```

### è¾“å‡ºæ ¼å¼ï¼ˆè®­ç»ƒæ ·æœ¬ï¼‰

```json
{
  "messages": [
    {
      "role": "user",
      "content": "You are a Lean4 theorem prover. Continue the proof...\n\nTheorem:\ntheorem add_comm...\n\nPrevious progress:\nintro a b\n...\n<continue>\n\nContinue the proof:"
    },
    {
      "role": "assistant",
      "content": "<proof>\ninduction a with\n| zero => simp\n</proof>"
    }
  ],
  "metadata": {
    "chunk_id": 1,
    "total_chunks": 3,
    "is_first_chunk": false,
    "is_last_chunk": false,
    "example_id": 0
  }
}
```

## ğŸ”§ ä¸‰ç§åˆ‡åˆ†ç­–ç•¥

### 1. æŒ‰è¡Œåˆ‡åˆ†ï¼ˆline_basedï¼‰- é»˜è®¤

- å°†è¯æ˜å‡åŒ€åˆ‡æˆ N å—
- ç®€å•å¯é 
- é€‚åˆå¿«é€ŸéªŒè¯

```yaml
chunking:
  strategy: "line_based"
  line_based:
    target_chunks: 3        # åˆ‡æˆ3å—
```

### 2. æŒ‰è¯­æ³•åˆ‡åˆ†ï¼ˆsyntax_basedï¼‰

- åœ¨ Lean4 å…³é”®è¯å¤„åˆ‡åˆ†ï¼ˆhave, cases, inductionç­‰ï¼‰
- æ›´ç¬¦åˆè¯­ä¹‰è¾¹ç•Œ
- éœ€è¦è¯†åˆ« Lean4 è¯­æ³•

```yaml
chunking:
  strategy: "syntax_based"
  syntax_based:
    target_chunk_size: 30
    split_keywords:
      - "have"
      - "cases"
      - "induction"
```

### 3. æŒ‰tokenåˆ‡åˆ†ï¼ˆtoken_basedï¼‰

- å›ºå®šæ¯å— token æ•°
- é€‚åˆæ§åˆ¶è¾“å…¥é•¿åº¦
- å¯é…ç½®é‡å 

```yaml
chunking:
  strategy: "token_based"
  token_based:
    tokens_per_chunk: 2048
    overlap: 100
```

## ğŸ“ˆ æ•°æ®ç»Ÿè®¡ç¤ºä¾‹

è¿è¡Œåä¼šæ˜¾ç¤ºç±»ä¼¼è¾“å‡ºï¼š

```
ğŸ“Š Overall:
  Total samples: 1458
  Total proofs:  486

ğŸ”¢ Chunks per proof:
  Mean: 3.0 Â± 0.8
  Min:  1
  Max:  5

ğŸ“ Chunk lengths (lines):
  Mean: 12.5 Â± 8.3
  Min:  2
  Max:  48

ğŸ“ Token counts (words):
  Avg input:  156 words
  Avg output: 45 words

ğŸ¯ Chunk positions:
  First chunks:  486 (33.3%)
  Middle chunks: 486 (33.3%)
  Last chunks:   486 (33.3%)
```

## ğŸ§ª æµ‹è¯•å•ä¸ªæ¨¡å—

```bash
# æµ‹è¯•æ•°æ®ä¸‹è½½
cd data_preparation
python download_data.py

# æµ‹è¯•åˆ‡åˆ†é€»è¾‘
python chunk_proofs.py

# æµ‹è¯•æ ·æœ¬æ„é€ 
python build_training_data.py

# æµ‹è¯•æ•°æ®åˆ†æ
python analyze_data.py
```

## ğŸ¯ ä¸‹ä¸€æ­¥ï¼šæ¨¡å‹è®­ç»ƒ

ä½¿ç”¨ç”Ÿæˆçš„ `train.jsonl` å’Œ `val.jsonl` è¿›è¡Œ SFT è®­ç»ƒï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import SFTTrainer
from datasets import load_dataset

# åŠ è½½æ•°æ®
dataset = load_dataset('json', data_files={
    'train': 'data/processed/train.jsonl',
    'validation': 'data/processed/val.jsonl'
})

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "internlm/internlm2-math-plus-1_8b"
)
tokenizer = AutoTokenizer.from_pretrained(
    "internlm/internlm2-math-plus-1_8b"
)

# è®­ç»ƒ
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset['train'],
    eval_dataset=dataset['validation'],
    # ... å…¶ä»–å‚æ•°
)

trainer.train()
```

## âš™ï¸ é«˜çº§é…ç½®

### è¿‡æ»¤è§„åˆ™

```yaml
filtering:
  min_proof_length: 10      # æœ€çŸ­è¯æ˜é•¿åº¦
  max_proof_length: 200     # æœ€é•¿è¯æ˜é•¿åº¦
  skip_empty_proofs: true   # è·³è¿‡ç©ºè¯æ˜
  skip_sorry: true          # è·³è¿‡æœªå®Œæˆè¯æ˜ï¼ˆå«sorryï¼‰
```

### æç¤ºæ¨¡æ¿

ç¼–è¾‘ `config.yaml` ä¸­çš„ `prompt_template` è‡ªå®šä¹‰è¾“å…¥æ ¼å¼ï¼š

```yaml
training:
  prompt_template: |
    You are a Lean4 theorem prover.

    Theorem: {theorem}
    Context: {context}

    Continue the proof:
```

### ç‰¹æ®Šæ ‡è®°

```yaml
training:
  special_tokens:
    proof_start: "<proof>"
    proof_end: "</proof>"
    chunk_sep: "<chunk>"
    continue_tag: "<continue>"
```

## ğŸ“ å¸¸è§é—®é¢˜

### Q1: æ•°æ®ä¸‹è½½å¤±è´¥ï¼Ÿ

**A:** ç¡®ä¿æœ‰ç½‘ç»œè¿æ¥ï¼ŒHuggingFace æ•°æ®é›†éœ€è¦è®¿é—® `huggingface.co`ã€‚

### Q2: åˆ‡åˆ†ç»“æœä¸ç†æƒ³ï¼Ÿ

**A:** å°è¯•ä¸åŒçš„åˆ‡åˆ†ç­–ç•¥ï¼š
- `line_based`ï¼šæœ€ç¨³å®š
- `syntax_based`ï¼šæ›´æ™ºèƒ½ä½†å¯èƒ½å¤±è´¥
- `token_based`ï¼šå›ºå®šé•¿åº¦

### Q3: å†…å­˜ä¸è¶³ï¼Ÿ

**A:** å‡å°‘ `target_chunks` æˆ–å¢åŠ  `min_lines_per_chunk`ã€‚

### Q4: æƒ³è¦æ›´å¤šè®­ç»ƒæ•°æ®ï¼Ÿ

**A:**
- ä¿®æ”¹ `filtering` æ”¾å®½è¿‡æ»¤æ¡ä»¶
- ä½¿ç”¨ test splitï¼š`dataset.splits: ["valid", "test"]`

## ğŸ“š ä»£ç ç»“æ„

```
code/
â”œâ”€â”€ config.yaml                   # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt              # ä¾èµ–
â”œâ”€â”€ run_pipeline.py              # ä¸€é”®è¿è¡Œè„šæœ¬
â”œâ”€â”€ README.md                    # æœ¬æ–‡æ¡£
â”‚
â”œâ”€â”€ data_preparation/            # æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ download_data.py         # æ•°æ®ä¸‹è½½
â”‚   â”œâ”€â”€ chunk_proofs.py          # è¯æ˜åˆ‡åˆ†
â”‚   â”œâ”€â”€ build_training_data.py   # æ ·æœ¬æ„é€ 
â”‚   â””â”€â”€ analyze_data.py          # æ•°æ®åˆ†æ
â”‚
â”œâ”€â”€ data/                        # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ raw/                     # åŸå§‹æ•°æ®
â”‚   â””â”€â”€ processed/               # è®­ç»ƒæ•°æ®
â”‚
â””â”€â”€ output/                      # å…¶ä»–è¾“å‡º
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ PRï¼

## ğŸ“„ è®¸å¯

Apache 2.0 License

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€**
