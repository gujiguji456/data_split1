#!/usr/bin/env python3
"""
ä¸€é”®è¿è¡Œå®Œæ•´çš„æ•°æ®å‡†å¤‡æµç¨‹
"""

import sys
import argparse
from pathlib import Path
import yaml

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from data_preparation.download_data import MiniF2FLoader
from data_preparation.build_training_data import TrainingDataBuilder
from data_preparation.analyze_data import DataAnalyzer


def run_download(config):
    """æ­¥éª¤1: ä¸‹è½½æ•°æ®"""
    print("\n" + "="*60)
    print("STEP 1: Downloading MiniF2F-Lean4 Dataset")
    print("="*60)

    loader = MiniF2FLoader(config)

    # ä¸‹è½½æ•°æ®
    data = loader.download()

    # ä¿å­˜åŸå§‹æ•°æ®
    loader.save_raw_data(data)

    # è¿‡æ»¤æ•°æ®
    for split in data.keys():
        print(f"\n{'='*60}")
        print(f"Filtering {split} split")
        print('='*60)

        filtered = loader.filter_data(data[split])

        # ä¿å­˜è¿‡æ»¤åçš„æ•°æ®
        output_file = loader.cache_dir / f"{split}_filtered.jsonl"
        import json
        with open(output_file, 'w', encoding='utf-8') as f:
            for ex in filtered:
                f.write(json.dumps(ex, ensure_ascii=False) + '\n')

        print(f"\nâœ“ Saved filtered data to {output_file}")

    print("\nâœ… Step 1 completed!")


def run_build_training_data(config):
    """æ­¥éª¤2: æ„é€ è®­ç»ƒæ•°æ®"""
    print("\n" + "="*60)
    print("STEP 2: Building Training Data")
    print("="*60)

    loader = MiniF2FLoader(config)
    builder = TrainingDataBuilder(config)

    output_dir = Path(config['training']['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)

    for split in config['dataset']['splits']:
        print(f"\n{'='*60}")
        print(f"Processing {split} split")
        print('='*60)

        # åŠ è½½è¿‡æ»¤åçš„æ•°æ®
        try:
            import json
            filtered_file = loader.cache_dir / f"{split}_filtered.jsonl"

            if not filtered_file.exists():
                print(f"âš ï¸  Filtered file not found: {filtered_file}")
                print(f"ğŸ’¡ Skipping {split}")
                continue

            examples = []
            with open(filtered_file, 'r', encoding='utf-8') as f:
                for line in f:
                    examples.append(json.loads(line))

            print(f"ğŸ“– Loaded {len(examples)} filtered examples")

        except Exception as e:
            print(f"âŒ Error loading data: {e}")
            continue

        # æ„é€ è®­ç»ƒæ ·æœ¬
        samples = builder.build_dataset(examples)

        # ä¿å­˜å®Œæ•´æ•°æ®é›†
        output_file = output_dir / f"{split}_training.jsonl"
        builder.save_dataset(
            samples,
            output_file,
            format=config['training']['output_format']
        )

        # å¦‚æœæ˜¯ valid splitï¼Œè¿›ä¸€æ­¥åˆ†å‰²ä¸º train/val
        if split == "valid":
            train_samples, val_samples = builder.split_train_val(
                samples,
                ratio=config['training']['train_split_ratio']
            )

            train_file = output_dir / "train.jsonl"
            val_file = output_dir / "val.jsonl"

            builder.save_dataset(train_samples, train_file, format='jsonl')
            builder.save_dataset(val_samples, val_file, format='jsonl')

    print("\nâœ… Step 2 completed!")


def run_analyze(config):
    """æ­¥éª¤3: æ•°æ®åˆ†æ"""
    print("\n" + "="*60)
    print("STEP 3: Analyzing Data")
    print("="*60)

    analyzer = DataAnalyzer(config)
    data_dir = Path(config['training']['output_dir'])

    # åˆ†ææ‰€æœ‰æ•°æ®æ–‡ä»¶
    for data_file in sorted(data_dir.glob("*.jsonl")):
        print(f"\n{'='*60}")
        print(f"Analyzing: {data_file.name}")
        print('='*60)

        # åŠ è½½æ•°æ®
        samples = analyzer.load_samples(data_file)

        # è®¡ç®—ç»Ÿè®¡
        stats = analyzer.compute_statistics(samples)

        # æ‰“å°ç»Ÿè®¡
        analyzer.print_statistics(stats, title=f"Statistics: {data_file.stem}")

        # ç»˜åˆ¶åˆ†å¸ƒå›¾
        if config['analysis']['plot_distributions']:
            plot_dir = data_dir / "plots" / data_file.stem
            analyzer.plot_distributions(stats, plot_dir)

        # å±•ç¤ºæ ·æœ¬
        if config['analysis']['generate_stats']:
            analyzer.show_samples(samples, n=config['analysis']['sample_size'])

    print("\nâœ… Step 3 completed!")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Lean4 Delethink Data Preparation Pipeline"
    )

    parser.add_argument(
        '--config',
        type=str,
        default='config.yaml',
        help='Path to config file (default: config.yaml)'
    )

    parser.add_argument(
        '--steps',
        type=str,
        default='all',
        help='Steps to run: all, download, build, analyze (comma-separated)'
    )

    parser.add_argument(
        '--strategy',
        type=str,
        choices=['line_based', 'syntax_based', 'token_based'],
        help='Chunking strategy (overrides config)'
    )

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    config_path = Path(args.config)
    if not config_path.exists():
        print(f"âŒ Config file not found: {config_path}")
        return

    with open(config_path) as f:
        config = yaml.safe_load(f)

    # è¦†ç›–ç­–ç•¥
    if args.strategy:
        config['chunking']['strategy'] = args.strategy
        print(f"ğŸ”§ Using chunking strategy: {args.strategy}")

    # ç¡®å®šè¦è¿è¡Œçš„æ­¥éª¤
    if args.steps == 'all':
        steps = ['download', 'build', 'analyze']
    else:
        steps = [s.strip() for s in args.steps.split(',')]

    print("\n" + "="*60)
    print("Lean4 Delethink Data Preparation Pipeline")
    print("="*60)
    print(f"\nğŸ“‹ Steps to run: {', '.join(steps)}")
    print(f"ğŸ”§ Chunking strategy: {config['chunking']['strategy']}")

    # è¿è¡Œæ­¥éª¤
    try:
        if 'download' in steps:
            run_download(config)

        if 'build' in steps:
            run_build_training_data(config)

        if 'analyze' in steps:
            run_analyze(config)

        print("\n" + "="*60)
        print("ğŸ‰ Pipeline completed successfully!")
        print("="*60)

        # è¾“å‡ºä½ç½®
        output_dir = Path(config['training']['output_dir'])
        print(f"\nğŸ“ Output files:")
        print(f"  Data: {output_dir}")
        if (output_dir / "plots").exists():
            print(f"  Plots: {output_dir / 'plots'}")

        # ä¸‹ä¸€æ­¥æç¤º
        print(f"\nğŸ’¡ Next steps:")
        print(f"  1. Review the generated samples in {output_dir}")
        print(f"  2. Check the statistics and plots")
        print(f"  3. Use train.jsonl and val.jsonl for model training")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ Pipeline failed with error:")
        print(f"   {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
